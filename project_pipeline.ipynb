{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melissatorgbi/LLM-Clinical-Guideline-Understandability/blob/main/project_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ],
      "metadata": {
        "id": "MwE8xDBP2Epr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip --quiet install openai bert-score readability textstat python-dotenv boto3[crt]"
      ],
      "metadata": {
        "id": "dGm4J16ZYysz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip project.zip"
      ],
      "metadata": {
        "id": "WNs2oxiy2DMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p .aws\n",
        "!cp -r project/aws/* .aws/"
      ],
      "metadata": {
        "id": "INhiGSps1P9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv .aws/config.txt .aws/config\n",
        "!mv .aws/credentials.txt .aws/credentials"
      ],
      "metadata": {
        "id": "lzhv38dZ1W8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open( \"project/aws/credentials.txt\" ) as infile :\n",
        "  data = infile.readlines()\n",
        "aws_access_key_id = data[1].split( \"aws_access_key_id = \")[1].rstrip()\n",
        "aws_secret_access_key = data[2].split( \"aws_secret_access_key = \")[1].rstrip()"
      ],
      "metadata": {
        "id": "P_Zit4Jg2VhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24HxT7pZXV3t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import readability\n",
        "import textstat\n",
        "import nltk\n",
        "from bert_score import score\n",
        "from pprint import pprint\n",
        "from botocore.exceptions import ClientError\n",
        "import openai\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "import boto3\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from html.parser import HTMLParser\n",
        "from nltk.stem.porter import *\n",
        "nltk.download('punkt')\n",
        "#from evaluate import load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJHrNST5XV3y"
      },
      "outputs": [],
      "source": [
        "load_dotenv(find_dotenv(\"project/openai_api_keys.env\"))\n",
        "\n",
        "openai.api_key = os.environ.get('OPENAI_API_KEY')\n",
        "openai.organization = ''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setup aws comprehend api\n",
        "\n",
        "s3_client = boto3.client(\n",
        "  service_name='comprehend',\n",
        "  region_name=\"us-east-1\",\n",
        "  aws_access_key_id=aws_access_key_id,\n",
        "  aws_secret_access_key= aws_secret_access_key,\n",
        "  )\n",
        "\n",
        "medical_client = boto3.client(service_name='comprehendmedical',\n",
        "                              region_name='eu-west-2',\n",
        "                              aws_access_key_id=aws_access_key_id,\n",
        "                              aws_secret_access_key= aws_secret_access_key,)#,endpoint_url=endpoint_url)\n"
      ],
      "metadata": {
        "id": "9W-OpvWC13Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gpt_output(prompt):\n",
        "  client = OpenAI()\n",
        "  response = client.chat.completions.create(\n",
        "          #model = \"gpt-3.5-turbo-1106\",\n",
        "          model = \"gpt-4\",\n",
        "          messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "          #max_tokens = 2500,\n",
        "          )\n",
        "\n",
        "  gpt_output = response.choices[0].message.content\n",
        "  return gpt_output"
      ],
      "metadata": {
        "id": "NS7sihTwuhk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvJZDW5sXV3v"
      },
      "source": [
        "# Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rf1WMwXjXV3w"
      },
      "outputs": [],
      "source": [
        "def evaluate_readability(text):\n",
        "    fk = {\"flesch_score\": textstat.flesch_kincaid_grade(text),\n",
        "          \"sentence_count\": textstat.sentence_count(text)}\n",
        "    return fk #round(fk,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpdZJHAwXV3x"
      },
      "outputs": [],
      "source": [
        "def evaluate_BERT(original, new):\n",
        "    P, R, F1 = score([original], [new], lang=\"en\", verbose=False)\n",
        " #   row.append(F1.item())\n",
        "    print(\"BERT F1 is \", round(F1.item()),2)\n",
        "    return round(F1.item(),2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHJOBS2UXV3w"
      },
      "source": [
        "## Entities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_missing(string_list, target_string):\n",
        "    missing_strings = []\n",
        "    for string in string_list:\n",
        "        if string not in target_string:\n",
        "            missing_strings.append(string)\n",
        "    return missing_strings"
      ],
      "metadata": {
        "id": "g6IokuGTuPhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhlAkVbQXV3w"
      },
      "outputs": [],
      "source": [
        "def remove_stems(text):\n",
        "    # Tokenize the input text into individual words\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Initialize the PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Stem each word in the text\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Reconstruct the stemmed words back into a string\n",
        "    stemmed_text = \" \".join(stemmed_words)\n",
        "\n",
        "    return stemmed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p4ceqOZXV3w"
      },
      "outputs": [],
      "source": [
        "def remove_non_alphanumeric(text):\n",
        "    # Define a regular expression pattern to match non-alphanumeric characters (letters, numbers, and % sign)\n",
        "    pattern = r'[^a-zA-Z0-9\\s.%]+'\n",
        "\n",
        "    # Use re.sub() to replace the matching pattern with an empty string\n",
        "    cleaned_text = re.sub(pattern, '', text)\n",
        "\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw_2qP5IXV3w"
      },
      "outputs": [],
      "source": [
        "# this uses amazon comprehend to check that entities are retained in the improved versionbb\n",
        "def standard_entity_check(text):\n",
        "    #print(\"Checking this text for entities: \", text)\n",
        "    entity_text = []\n",
        "    #print(\"Detecting  general entities.\")\n",
        "   # response = medical_client.detect_entities_v2(Text=text)#, LanguageCode='en')\n",
        "    response = s3_client.detect_entities(Text=text, LanguageCode='en')\n",
        "    entities = response['Entities']\n",
        "  #  print(entities)\n",
        "    for entity in entities:\n",
        "       # print(f'Type: {entity[\"Type\"]}, Text: {entity[\"Text\"]}')\n",
        "        entity_text.append(entity[\"Text\"].lower()) #make sure decapitalised\n",
        "    return entity_text, entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hwy6PidkXV3x"
      },
      "outputs": [],
      "source": [
        "#Medical Named Entity and Relationship Extraction (NERe)\n",
        "# this uses amazon comprehend to check that entities are retained in the improved versionbb\n",
        "def medical_entity_check(text):\n",
        "    #print(\"Checking this text for entities: \", text)\n",
        "    entity_text = []\n",
        "    #print(\"Detecting medical entities.\")\n",
        "    response = medical_client.detect_entities_v2(Text=text)\n",
        "    entities = response['Entities']\n",
        "  #  print(entities)\n",
        "    for entity in entities:\n",
        "        #print(f'Type: {entity[\"Type\"]}, Text: {entity[\"Text\"]}')\n",
        "        entity_text.append(entity[\"Text\"].lower()) #make sure decapitalised\n",
        "    return entity_text, entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBlGkY5OXV3x"
      },
      "outputs": [],
      "source": [
        "def extact_all_entities(text):\n",
        "    standard_ent_text, standard_entities = standard_entity_check(text)\n",
        "    med_ent_text, med_entities = medical_entity_check(text)\n",
        "    all_entities = standard_ent_text + med_ent_text\n",
        "    return all_entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMRy8S46XV3x"
      },
      "outputs": [],
      "source": [
        "def reformat_entiites(entity_list):\n",
        "    entity_list = list(map(remove_non_alphanumeric, entity_list))\n",
        "    entity_list = list(map(str.lower, entity_list))\n",
        "    entity_list = list(map(remove_stems, entity_list))\n",
        "    return entity_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSRuvHSFXV3x"
      },
      "outputs": [],
      "source": [
        "def evaluate_entities(original, new):\n",
        "    all_ori_entities = extact_all_entities(original)\n",
        "    all_new_entities = extact_all_entities(new)\n",
        "\n",
        "    # process entities to remove stems, capitalisation and grammatical symbols\n",
        "    all_ori_entities = reformat_entiites(all_ori_entities)\n",
        "    all_new_entities = reformat_entiites(all_new_entities)\n",
        "\n",
        "\n",
        "\n",
        "    missing = find_missing(all_ori_entities, all_new_entities)\n",
        "    #print(\"OLD\", all_ori_entities)\n",
        "    #print(\"NEW\", all_ori_entities)\n",
        "    extra = find_missing(all_new_entities, all_ori_entities)\n",
        "    return all_ori_entities, all_new_entities, missing, extra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KPeqX3rXV3x"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PROMPTS\n",
        "\n",
        "prompt_file = open('project/readability_prompt.txt', \"r\")\n",
        "readability_prompt = prompt_file.read()\n",
        "\n",
        "prompt_file = open('project/formatting_prompt.txt', \"r\")\n",
        "formatting_prompt = prompt_file.read()\n",
        "\n",
        "questions_prompt = \"\"\"You are a nurse administering IV medication,\n",
        "what questions would you need to be ask a technical expert, to generate the following text?\n",
        "Give the output directly as a list of numbered questions - TEXT: \"\"\"\n",
        "\n",
        "suggestions_prompt = '''You are a nurse following a guideline for administering IV medication. You need to find questions from the list provided\n",
        "which are NOT already answered by the guideline you are given. Find the unanswered questions.\n",
        "As well as the question, output section heading from the guideline most related to the question.\n",
        "The output format will be the heading: followed by the questions.\n",
        "Only output a maximum of ten questions in total across all sections.\n",
        "These are the questions '''"
      ],
      "metadata": {
        "id": "zFfMR9fgXP24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir results"
      ],
      "metadata": {
        "id": "zGMsh7hx6fkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/project/medications/\"\n",
        "\n",
        "for file in os.listdir(path):\n",
        "\n",
        "  if file.endswith(\".md\"):\n",
        "    md_file = open(path+file, \"rb\")\n",
        "\n",
        "    original_content = str(md_file.read())\n",
        "    medication = file.split('.')[0]\n",
        "\n",
        "\n",
        "    #READABILITY\n",
        "\n",
        "    gpt_readability = get_gpt_output(readability_prompt +\"\\n\" + original_content)\n",
        "    gpt_readability = gpt_readability.strip(\"```\").strip(\"markdown\")\n",
        "\n",
        "    bert_similarity = evaluate_BERT(original_content, gpt_readability)\n",
        "    original_metrics = evaluate_readability(original_content)\n",
        "    new_metrics = evaluate_readability(gpt_readability)\n",
        "    #all_ori_entities, all_gpt_entities, gpt_missing_entities, gpt_extra_entities = evaluate_entities(original_text, new_text)\n",
        "\n",
        "\n",
        "    #FORMATTING\n",
        "\n",
        "    gpt_formatting = get_gpt_output(formatting_prompt +\"\\n\" + gpt_readability)\n",
        "\n",
        "    f = open(\"results/\" + medication + \"_gpt\" + \".md\", \"a\")\n",
        "    f.write(gpt_formatting.strip(\"```\").strip(\"markdown\"))\n",
        "    f.close()\n",
        "\n",
        "    #TECHNICAL QUESTIONS AND SUGGESTIONS\n",
        "    gpt_questions = get_gpt_output(questions_prompt +\"\\n\" + original_content)\n",
        "    gpt_suggestions = get_gpt_output(suggestions_prompt + gpt_questions + '. This is the guideline text: ' +  original_content)\n",
        "\n",
        "    #SAVE OUTPUTS\n",
        "    f = open(\"results/\" + medication + \".txt\", \"a\")\n",
        "    f.write(\n",
        "        \"BERT Similarity score: \" +str(bert_similarity)\n",
        "        +\"\\nOriginal text flesch-kincaid grade level: \"+str(original_metrics['flesch_score'])\n",
        "        +\"\\nOriginal text sentence count: \"+str(original_metrics['sentence_count'])\n",
        "        +\"\\ngpt output flesch-kincaid grade level: \"+str(new_metrics['flesch_score'])\n",
        "        +\"\\ngpt output sentence count: \"+str(new_metrics['sentence_count'])\n",
        "        #+\"\\nAll entities from original text: \"+str(all_ori_entities)\n",
        "        #+\"\\nAll entities from gpt output: \"+str(all_gpt_entities)\n",
        "        #+\"\\nMissing entities in gpt output: \"+str(gpt_missing_entities)\n",
        "        #+\"\\nAdditional entities in gpt output: \"+str(gpt_extra_entities)\n",
        "        +\"\\n\\nTechnical Suggestions\\n\\n\"+str(gpt_suggestions)\n",
        "        )\n",
        "\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "gkt80fGCjQ_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r results.zip results"
      ],
      "metadata": {
        "id": "9PrfmTap2K9D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "MwE8xDBP2Epr",
        "gvJZDW5sXV3v"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}